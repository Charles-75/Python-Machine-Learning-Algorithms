{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>5</th>\n",
       "      <th>874965758</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>875071561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  1.1  5  874965758\n",
       "0  1    2  3  876893171\n",
       "1  1    3  4  878542960\n",
       "2  1    4  3  876893119\n",
       "3  1    5  3  889751712\n",
       "4  1    7  4  875071561"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "\n",
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter='\\t')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter='\\t')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "training_set = np.array(training_set, dtype='int')\n",
    "test_set = np.array(test_set, dtype='int')\n",
    "\n",
    "# get the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "print(nb_users)\n",
    "print(nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data into a matrix with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_user in range(1, nb_users+1):\n",
    "        id_movies = data[:,1][data[:,0] == id_user]\n",
    "        id_ratings = data[:,2][data[:,0] == id_user]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies-1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "            \n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)    \n",
    "\n",
    "# converting data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = StackedAutoEncoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(auto_encoder.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7658158540725708\n",
      "epoch: 2 loss: 1.0965160131454468\n",
      "epoch: 3 loss: 1.0534982681274414\n",
      "epoch: 4 loss: 1.0381323099136353\n",
      "epoch: 5 loss: 1.030817985534668\n",
      "epoch: 6 loss: 1.0267314910888672\n",
      "epoch: 7 loss: 1.023602843284607\n",
      "epoch: 8 loss: 1.0220305919647217\n",
      "epoch: 9 loss: 1.0207315683364868\n",
      "epoch: 10 loss: 1.0194804668426514\n",
      "epoch: 11 loss: 1.0189080238342285\n",
      "epoch: 12 loss: 1.0183860063552856\n",
      "epoch: 13 loss: 1.0178974866867065\n",
      "epoch: 14 loss: 1.0175381898880005\n",
      "epoch: 15 loss: 1.0171769857406616\n",
      "epoch: 16 loss: 1.0170212984085083\n",
      "epoch: 17 loss: 1.0167031288146973\n",
      "epoch: 18 loss: 1.0165205001831055\n",
      "epoch: 19 loss: 1.0162731409072876\n",
      "epoch: 20 loss: 1.016390323638916\n",
      "epoch: 21 loss: 1.0160131454467773\n",
      "epoch: 22 loss: 1.0159878730773926\n",
      "epoch: 23 loss: 1.0157746076583862\n",
      "epoch: 24 loss: 1.0156831741333008\n",
      "epoch: 25 loss: 1.0157876014709473\n",
      "epoch: 26 loss: 1.0157089233398438\n",
      "epoch: 27 loss: 1.0152873992919922\n",
      "epoch: 28 loss: 1.015108346939087\n",
      "epoch: 29 loss: 1.012308955192566\n",
      "epoch: 30 loss: 1.0110862255096436\n",
      "epoch: 31 loss: 1.0093655586242676\n",
      "epoch: 32 loss: 1.008742332458496\n",
      "epoch: 33 loss: 1.0044831037521362\n",
      "epoch: 34 loss: 1.005568027496338\n",
      "epoch: 35 loss: 1.0008976459503174\n",
      "epoch: 36 loss: 0.9999609589576721\n",
      "epoch: 37 loss: 0.9978764653205872\n",
      "epoch: 38 loss: 0.9968268275260925\n",
      "epoch: 39 loss: 0.9922152161598206\n",
      "epoch: 40 loss: 0.9930728673934937\n",
      "epoch: 41 loss: 0.988648533821106\n",
      "epoch: 42 loss: 0.991184413433075\n",
      "epoch: 43 loss: 0.9895951151847839\n",
      "epoch: 44 loss: 0.98606938123703\n",
      "epoch: 45 loss: 0.9825776219367981\n",
      "epoch: 46 loss: 0.9803366661071777\n",
      "epoch: 47 loss: 0.9764341711997986\n",
      "epoch: 48 loss: 0.9757859110832214\n",
      "epoch: 49 loss: 0.9720844626426697\n",
      "epoch: 50 loss: 0.973102867603302\n",
      "epoch: 51 loss: 0.9701683521270752\n",
      "epoch: 52 loss: 0.9679577946662903\n",
      "epoch: 53 loss: 0.9645514488220215\n",
      "epoch: 54 loss: 0.9648478031158447\n",
      "epoch: 55 loss: 0.9619128108024597\n",
      "epoch: 56 loss: 0.9691188931465149\n",
      "epoch: 57 loss: 0.9646890759468079\n",
      "epoch: 58 loss: 0.9611242413520813\n",
      "epoch: 59 loss: 0.9603254795074463\n",
      "epoch: 60 loss: 0.9602972269058228\n",
      "epoch: 61 loss: 0.9582381844520569\n",
      "epoch: 62 loss: 0.9587385058403015\n",
      "epoch: 63 loss: 0.9573431015014648\n",
      "epoch: 64 loss: 0.9602558016777039\n",
      "epoch: 65 loss: 0.9559242129325867\n",
      "epoch: 66 loss: 0.9588331580162048\n",
      "epoch: 67 loss: 0.9562764763832092\n",
      "epoch: 68 loss: 0.9569251537322998\n",
      "epoch: 69 loss: 0.9549183249473572\n",
      "epoch: 70 loss: 0.955976128578186\n",
      "epoch: 71 loss: 0.9530744552612305\n",
      "epoch: 72 loss: 0.9545904994010925\n",
      "epoch: 73 loss: 0.9508101344108582\n",
      "epoch: 74 loss: 0.9510937333106995\n",
      "epoch: 75 loss: 0.948528528213501\n",
      "epoch: 76 loss: 0.9480056762695312\n",
      "epoch: 77 loss: 0.9457708597183228\n",
      "epoch: 78 loss: 0.9467215538024902\n",
      "epoch: 79 loss: 0.9451533555984497\n",
      "epoch: 80 loss: 0.9520218372344971\n",
      "epoch: 81 loss: 0.9475148320198059\n",
      "epoch: 82 loss: 0.9597906470298767\n",
      "epoch: 83 loss: 0.951403021812439\n",
      "epoch: 84 loss: 0.9539306163787842\n",
      "epoch: 85 loss: 0.9523698091506958\n",
      "epoch: 86 loss: 0.9516181349754333\n",
      "epoch: 87 loss: 0.9491998553276062\n",
      "epoch: 88 loss: 0.952951967716217\n",
      "epoch: 89 loss: 0.9532889723777771\n",
      "epoch: 90 loss: 0.9536439180374146\n",
      "epoch: 91 loss: 0.9486238956451416\n",
      "epoch: 92 loss: 0.9477512240409851\n",
      "epoch: 93 loss: 0.9466122388839722\n",
      "epoch: 94 loss: 0.9486262202262878\n",
      "epoch: 95 loss: 0.9467846155166626\n",
      "epoch: 96 loss: 0.9483622908592224\n",
      "epoch: 97 loss: 0.9421765208244324\n",
      "epoch: 98 loss: 0.9420027732849121\n",
      "epoch: 99 loss: 0.9409474730491638\n",
      "epoch: 100 loss: 0.9406874775886536\n",
      "epoch: 101 loss: 0.9414170980453491\n",
      "epoch: 102 loss: 0.9419506192207336\n",
      "epoch: 103 loss: 0.9377819299697876\n",
      "epoch: 104 loss: 0.9392542243003845\n",
      "epoch: 105 loss: 0.9359713792800903\n",
      "epoch: 106 loss: 0.9403027892112732\n",
      "epoch: 107 loss: 0.9398073554039001\n",
      "epoch: 108 loss: 0.9384329915046692\n",
      "epoch: 109 loss: 0.9362934827804565\n",
      "epoch: 110 loss: 0.9361167550086975\n",
      "epoch: 111 loss: 0.9345980882644653\n",
      "epoch: 112 loss: 0.9344165325164795\n",
      "epoch: 113 loss: 0.9329700469970703\n",
      "epoch: 114 loss: 0.9327067136764526\n",
      "epoch: 115 loss: 0.9331116080284119\n",
      "epoch: 116 loss: 0.9309905767440796\n",
      "epoch: 117 loss: 0.9298515319824219\n",
      "epoch: 118 loss: 0.9307959675788879\n",
      "epoch: 119 loss: 0.9289768934249878\n",
      "epoch: 120 loss: 0.9296958446502686\n",
      "epoch: 121 loss: 0.928215742111206\n",
      "epoch: 122 loss: 0.9281485080718994\n",
      "epoch: 123 loss: 0.9272933006286621\n",
      "epoch: 124 loss: 0.9277276396751404\n",
      "epoch: 125 loss: 0.9270632266998291\n",
      "epoch: 126 loss: 0.926669716835022\n",
      "epoch: 127 loss: 0.9258487820625305\n",
      "epoch: 128 loss: 0.9264799952507019\n",
      "epoch: 129 loss: 0.9260410666465759\n",
      "epoch: 130 loss: 0.9267566204071045\n",
      "epoch: 131 loss: 0.9247495532035828\n",
      "epoch: 132 loss: 0.9249750971794128\n",
      "epoch: 133 loss: 0.9236011505126953\n",
      "epoch: 134 loss: 0.9235675930976868\n",
      "epoch: 135 loss: 0.9228041768074036\n",
      "epoch: 136 loss: 0.9243298768997192\n",
      "epoch: 137 loss: 0.9236353039741516\n",
      "epoch: 138 loss: 0.9237189292907715\n",
      "epoch: 139 loss: 0.9225766062736511\n",
      "epoch: 140 loss: 0.9225812554359436\n",
      "epoch: 141 loss: 0.9218116402626038\n",
      "epoch: 142 loss: 0.9221050143241882\n",
      "epoch: 143 loss: 0.921237587928772\n",
      "epoch: 144 loss: 0.9212732315063477\n",
      "epoch: 145 loss: 0.9206628203392029\n",
      "epoch: 146 loss: 0.9216686487197876\n",
      "epoch: 147 loss: 0.9202100038528442\n",
      "epoch: 148 loss: 0.9208727478981018\n",
      "epoch: 149 loss: 0.9196507930755615\n",
      "epoch: 150 loss: 0.9200100898742676\n",
      "epoch: 151 loss: 0.919032096862793\n",
      "epoch: 152 loss: 0.9194316267967224\n",
      "epoch: 153 loss: 0.9183193445205688\n",
      "epoch: 154 loss: 0.9199190735816956\n",
      "epoch: 155 loss: 0.9182369709014893\n",
      "epoch: 156 loss: 0.9189518094062805\n",
      "epoch: 157 loss: 0.9175108671188354\n",
      "epoch: 158 loss: 0.9183006286621094\n",
      "epoch: 159 loss: 0.9175580739974976\n",
      "epoch: 160 loss: 0.918194055557251\n",
      "epoch: 161 loss: 0.9171183705329895\n",
      "epoch: 162 loss: 0.9177261590957642\n",
      "epoch: 163 loss: 0.9164444804191589\n",
      "epoch: 164 loss: 0.9170745611190796\n",
      "epoch: 165 loss: 0.9162286520004272\n",
      "epoch: 166 loss: 0.9165430665016174\n",
      "epoch: 167 loss: 0.9164494276046753\n",
      "epoch: 168 loss: 0.9163270592689514\n",
      "epoch: 169 loss: 0.9154868721961975\n",
      "epoch: 170 loss: 0.915910005569458\n",
      "epoch: 171 loss: 0.9153519868850708\n",
      "epoch: 172 loss: 0.9155706167221069\n",
      "epoch: 173 loss: 0.9149802923202515\n",
      "epoch: 174 loss: 0.9153211712837219\n",
      "epoch: 175 loss: 0.9147474765777588\n",
      "epoch: 176 loss: 0.9147260785102844\n",
      "epoch: 177 loss: 0.9144781827926636\n",
      "epoch: 178 loss: 0.914995551109314\n",
      "epoch: 179 loss: 0.914297342300415\n",
      "epoch: 180 loss: 0.9141376614570618\n",
      "epoch: 181 loss: 0.9138803482055664\n",
      "epoch: 182 loss: 0.913914144039154\n",
      "epoch: 183 loss: 0.9136760234832764\n",
      "epoch: 184 loss: 0.9136228561401367\n",
      "epoch: 185 loss: 0.9139523506164551\n",
      "epoch: 186 loss: 0.9147588610649109\n",
      "epoch: 187 loss: 0.9152253270149231\n",
      "epoch: 188 loss: 0.9133317470550537\n",
      "epoch: 189 loss: 0.9130630493164062\n",
      "epoch: 190 loss: 0.9128953814506531\n",
      "epoch: 191 loss: 0.9128761887550354\n",
      "epoch: 192 loss: 0.912534236907959\n",
      "epoch: 193 loss: 0.9126502871513367\n",
      "epoch: 194 loss: 0.9122457504272461\n",
      "epoch: 195 loss: 0.9120029211044312\n",
      "epoch: 196 loss: 0.9121521711349487\n",
      "epoch: 197 loss: 0.9117465019226074\n",
      "epoch: 198 loss: 0.9116188883781433\n",
      "epoch: 199 loss: 0.9112124443054199\n",
      "epoch: 200 loss: 0.9112322330474854\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 200\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    train_loss = 0\n",
    "    users_rated_count = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        #test that the user rated at least one movie (optimize memory)\n",
    "        if torch.sum(target.data > 0) > 0:  \n",
    "            output = auto_encoder(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
    "            users_rated_count += 1.\n",
    "            optimizer.step()\n",
    "    print(f'epoch: {epoch} loss: {train_loss/users_rated_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.9507325291633606\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "users_rated_count = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
    "    #test that the user rated at least one movie (optimize memory)\n",
    "    if torch.sum(target.data > 0) > 0:  \n",
    "        output = auto_encoder(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
    "        users_rated_count += 1.\n",
    "print(f'test loss: {test_loss/users_rated_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
